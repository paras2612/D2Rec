# -*- coding: utf-8 -*-
"""CIRS_Baseline.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1u4yWn45-UAelQxxhkQ8NFJQRJKghoqZD
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from random import randint
from collections import defaultdict
from sklearn.preprocessing import StandardScaler
import datetime
import csv
import numpy as np
import random
import torch
import torch.nn as nn
import warnings
import pandas as pd
import torch.nn.functional as F
from copy import deepcopy



ratings = pd.read_csv("/content/drive/MyDrive/Copy of Epinions/Data-final/ratings_data.csv")
#ratings = pd.read_csv("/content/drive/MyDrive/Copy of Epinions/Ciao/ratings_data.csv")
ratings = ratings[['userId', 'productId', 'rating']].values

print(len(np.unique(ratings[:, 0])), len(np.unique(ratings[:, 1])))
#train, test = train_test_split(ratings, test_size=0.2)  # spilt_rating_dat(ratings)
#train = np.loadtxt("/content/drive/MyDrive/Copy of Epinions/Data-final/user_ratings_final_train.txt")
#test = np.loadtxt("/content/drive/MyDrive/Copy of Epinions/Data-final/user_ratings_final_test.txt")
#train = np.loadtxt("/content/drive/MyDrive/Copy of Epinions/Data-final/user_ratings_unbiased_train.txt")
#print(train.shape)


train = pd.read_csv("/content/drive/MyDrive/Copy of Epinions/testData_Final/ratings_data_train.csv")
#train = pd.read_csv("/content/drive/MyDrive/Copy of Epinions/Ciao/ratings_data_train.csv")
train = train[["userId","productId","rating"]].values

from scipy.sparse import csc_matrix
mat_data = csc_matrix((len(np.unique(ratings[:, 0])), len(np.unique(ratings[:, 1]))), dtype=np.int8)
mat_data = mat_data.tolil()
print(mat_data.shape)
for uid,pid,_ in ratings:
  mat_data[uid,pid] = 1
mat_data = mat_data.tocoo()

from scipy.sparse import csc_matrix
mat_data_rate = csc_matrix((len(np.unique(ratings[:, 0])), len(np.unique(ratings[:, 1]))), dtype=np.int8)
mat_data_rate = mat_data_rate.tolil()
print(mat_data_rate.shape)
for uid,pid,_ in ratings:
  mat_data_rate[uid,pid] = _
mat_data_rate = mat_data_rate.tocoo()

import torch
from torch.autograd import Variable


class SigNet(torch.nn.Module):
  def __init__(self,n_users,n_items,n_features):
    #Get Embedding for user and item attributes
    super(SigNet, self).__init__()
    self.theta_u = torch.nn.Embedding(n_users,n_features)
    #self.theta_u = torch.nn.Parameter(embed_u,requires_grad = True)
    #self.beta_i = torch.nn.Parameter(embed_i,requires_grad = True)
    self.beta_i = torch.nn.Embedding(n_items,n_features)
    self.relu = torch.nn.ReLU()
    self.gamma = torch.nn.Parameter(torch.rand(n_users),requires_grad=True)

  def forward(self,user,items,true_exposure,pred_exposure):
    user = user.cuda()
    items = items.cuda()
    true_exposure = true_exposure.cuda()
    pred_exposure = pred_exposure.cuda()
    #self.theta_u = self.theta_u.cuda()
    #self.beta_i = self.beta_i.cuda()
    
    #user_values = [i.item() for i in user]
    #user_u = self.snea(user_values)
    #user_u = self.theta_u[user]
    #user_u = user_u.cuda()
    user_u = self.theta_u(user)
    item_i = self.beta_i(items)
    #item_i = self.beta_i[items]
    #item_i = item_i.cuda()
    
    gamma = self.gamma[user].cuda()
    y_pred = torch.matmul(user_u,item_i.T)*true_exposure + gamma*pred_exposure
    #y_pred = torch.matmul(user_u,item_i.T)*true_exposure
    #idx = torch.where(true_exposure>0)
    #y_pred = y_pred[idx[0],idx[1]]
    return self.relu(y_pred)

from torch.utils.data import DataLoader,Dataset

class UserItemRatingDataset(Dataset):
    """Wrapper, convert <user, item, rating> Tensor into Pytorch Dataset"""
    def __init__(self, user_tensor, item_tensor, target_tensor):
        """
        args:
            target_tensor: torch.Tensor, the corresponding rating for <user, item> pair
        """
        self.user_tensor = user_tensor
        self.item_tensor = item_tensor
        self.target_tensor = target_tensor

    def __getitem__(self, index):
      get_output = [self.user_tensor[index], self.item_tensor[index], self.target_tensor[index]]

      return get_output

    def __len__(self):
        return self.user_tensor.size(0)

def instance_a_train_loader(data, batch_size):
        users = []
        items = []
        ratings = []

        for row in data:
            users.append(int(row[0]))
            items.append(int(row[1]))
            ratings.append(float(row[2]))
        dataset = UserItemRatingDataset(user_tensor=torch.LongTensor(users),
                                        item_tensor=torch.LongTensor(items),
                                        target_tensor=torch.FloatTensor(ratings),
                                        )
        
        return DataLoader(dataset, batch_size=batch_size, shuffle=True)

train_loader = instance_a_train_loader(train,1000)

#data = np.loadtxt("/content/drive/MyDrive/Copy of Epinions/Ciao/data/cause_pmf_k64_U.csv")
#i_data = np.loadtxt("/content/drive/MyDrive/Copy of Epinions/Ciao/data/cause_pmf_k64_V.csv")
data = np.loadtxt("/content/drive/MyDrive/Copy of Epinions/data2/cause_pmf_k64_U.csv")
i_data = np.loadtxt("/content/drive/MyDrive/Copy of Epinions/data2/cause_pmf_k64_V.csv")
embed_user_s = torch.FloatTensor(data)
embed_user_s.shape

embed_item_s = torch.FloatTensor(i_data)
embed_item_s.shape

import dill
model = SigNet(len(np.unique(ratings[:, 0])), len(np.unique(ratings[:, 1])),64)
model = model.cuda()

import dill
model = SigNet(len(np.unique(ratings[:, 0])), len(np.unique(ratings[:, 1])),64)
model = model.cuda()
min_loss = 1e+5
optimizer = torch.optim.Adam(model.parameters(), lr = 1e-2)
loss_fn = torch.nn.MSELoss()
breakout = 0
for epoch in range(200):
  total_pred_loss = 0
  c=0
  for batch_num,batch in enumerate(train_loader):
    c+=1
    user = batch[0]
    item = batch[1]
    rating = batch[2]

    #rating_min = np.asarray([emb_i_values[i][0] for i in item])
    #rating_user = np.asarray([emb_values[u][0] for u in user])
    rating_min = embed_item_s[item]
    rating_user = embed_user_s[user]
    exp_item = torch.FloatTensor(np.stack(rating_min)).cuda()
    exp_user = torch.FloatTensor(np.stack(rating_user)).cuda()
    pred_exposure = torch.poisson(torch.mm(exp_user,exp_item.T))
    
    exp_batch = mat_data.tocsr()[user, :].tocsc()[:, item]
    exp_batch_tensor = torch.FloatTensor(np.stack(exp_batch.toarray())).cuda()


    rate_batch = mat_data_rate.tocsr()[user, :].tocsc()[:, item]
    rate_batch_tensor = torch.FloatTensor(np.stack(rate_batch.toarray())).cuda()
    idx = torch.where(rate_batch_tensor>0)
    rating_new = rate_batch_tensor[idx[0],idx[1]]


    y_pred = model.forward(user,item,exp_batch_tensor,pred_exposure)
    y_pred = y_pred[idx[0],idx[1]]
    batch_loss = loss_fn(y_pred,rating_new)
    batch_loss.backward()
    optimizer.step()
    optimizer.zero_grad()
    total_pred_loss += batch_loss.item()
  if(round(total_pred_loss/c,2)<round(min_loss/c,2)):
    min_loss = total_pred_loss
    breakout = 0
    checkpoint_model = deepcopy(model)
  else:
    breakout+=1
  if(breakout==10):
    break
  checkpoint = {'model': checkpoint_model,
          'state_dict': checkpoint_model.state_dict()
          }
  #if(epoch%10==0):
    #file = "/content/drive/MyDrive/Copy of Epinions/Models_Final/basic_model_without_network"+str(epoch)+"_"+str(1e-2)
    #file = "/content/drive/MyDrive/Copy of Epinions/Ciao/Models/basic_model_without_network"+str(epoch)+"_"+str(1e-2)
    #torch.save(pickle_module=dill,obj = checkpoint,f=file)
    #print("Model Saved")
  
  print("Train prediction loss is ", round((total_pred_loss/c),2)," for epoch ",epoch)

#model.load_state_dict(torch.load("/content/drive/MyDrive/Copy of Epinions/Models_Final/basic_model_without_network"+str(180)+"_"+str(1e-2),pickle_module=dill)['state_dict'])
loss_fn = torch.nn.MSELoss()
loss2_fn = torch.nn.L1Loss()
import os
#path = "/content/drive/MyDrive/Copy of Epinions/Ciao/"
path = "/content/drive/MyDrive/Copy of Epinions/testData_Final/"
files = os.listdir(path)
for f in files:
  if("test" in f):
    print(f)  
    test = pd.read_csv(path+f)
    test = test[["userId","productId","rating"]].values
    test_loader = instance_a_train_loader(test,100)
    #model = checkpoint_model
    model.eval()
    total_pred_loss = 0
    total_treat_loss = 0
    c=0
    for batch_num,batch in enumerate(test_loader):
        c+=1
        user = batch[0]
        item = batch[1]
        rating = batch[2]
        
        rating_min = embed_item_s[item]
        rating_user = embed_user_s[user]
        exp_item = torch.FloatTensor(np.stack(rating_min)).cuda()
        exp_user = torch.FloatTensor(np.stack(rating_user)).cuda()
        pred_exposure = torch.poisson(torch.mm(exp_user,exp_item.T))

        exp_batch = mat_data.tocsr()[user, :].tocsc()[:, item]
        exp_batch_tensor = torch.FloatTensor(np.stack(exp_batch.toarray())).cuda()


        rate_batch = mat_data_rate.tocsr()[user, :].tocsc()[:, item]
        rate_batch_tensor = torch.FloatTensor(np.stack(rate_batch.toarray())).cuda()
        idx = torch.where(rate_batch_tensor>0)
        rating_new = rate_batch_tensor[idx[0],idx[1]]


        y_pred = model.forward(user,item,exp_batch_tensor,pred_exposure)
        y_pred = y_pred[idx[0],idx[1]]
        batch_loss = loss_fn(y_pred,rating_new)
        total_pred_loss += batch_loss.item()
        batch_loss2 = loss2_fn(y_pred,rating_new)
        total_treat_loss += batch_loss2.item()
    print(round(total_pred_loss/(batch_num+1),2),round(total_treat_loss/(batch_num+1),2))

import os
import math
import warnings
warnings.filterwarnings("ignore")
def _sample_negative(item_map,ratings):
        random.seed(10)
        """return all negative items & 100 sampled negative items"""
        interact_status = ratings.groupby('userId')['productId'].apply(set).reset_index().rename(
            columns={'productId': 'interacted_items'})
        interact_status['negative_samples'] = interact_status['interacted_items'].apply(lambda x: random.sample(set(list(range(len(item_map)))) - x, 10))
        return interact_status[['userId', 'negative_samples']]
        #interact_status['negative_items'] = interact_status['interacted_items'].apply(lambda x: set(list(range(len(item_map)))) - x)
        #interact_status['negative_samples'] = interact_status['negative_items'].apply(lambda x: random.sample(x, 10))
        #return interact_status[['userId', 'negative_items', 'negative_samples']]

def cal_hit_ratio(subjects,topk=10):
        """Hit Ratio @ top_K"""
        full, top_k = subjects,topk
        top_k = full[full['rank']<=top_k]
        test_in_top_k =top_k[top_k['test_item'] == top_k['item']]  # golden items hit in the top_K items
        return len(test_in_top_k) * 1.0 / full['user'].nunique()

def cal_ndcg(subjects,topk=10):
        full, top_k = subjects,topk
        top_k = full[full['rank']<=top_k]
        test_in_top_k =top_k[top_k['test_item'] == top_k['item']]
        test_in_top_k['ndcg'] = test_in_top_k['rank'].apply(lambda x: math.log(2) / math.log(1 + x)) # the rank starts from 1
        return test_in_top_k['ndcg'].sum() * 1.0 / full['user'].nunique()



model.load_state_dict(torch.load("/content/drive/MyDrive/Copy of Epinions/Models_Final/basic_model_without_network"+str(180)+"_"+str(1e-2),pickle_module=dill)['state_dict'])
path = "/content/drive/MyDrive/Copy of Epinions/testData_Final/"
#path = "/content/drive/MyDrive/Copy of Epinions/Ciao/"
files = os.listdir(path)
loss_fn = torch.nn.MSELoss()
loss2_fn = torch.nn.L1Loss()
model.eval()
for f in files:
  if("test" in f):
    if(f=="user_ratings_test_new.csv"):
      continue
    print(f)  
    test = pd.read_csv(path+f)
    test = test[["userId","productId","rating"]]
    print(test.shape)
    test_negatives = _sample_negative(list(test["productId"].unique()),test)
    a = pd.merge(test, test_negatives[['userId', 'negative_samples']], on='userId')
    del test_negatives
    test_users,test_items,ratings,negative_users, negative_items,negative_ratings =[],[],[],[],[],[]
    for row in a.itertuples():
      test_users.append(int(row.userId))
      test_items.append(int(row.productId))
      ratings.append(float(row.rating))
      for i in range(len(row.negative_samples)):
          negative_users.append(int(row.userId))
          negative_items.append(int(row.negative_samples[i]))
          negative_ratings.append(float(0))  # negative samples get 0 rating
    del a
    tu,ti,tr,nu,ni,nr = torch.LongTensor(test_users), torch.LongTensor(test_items), torch.DoubleTensor(ratings), torch.LongTensor(negative_users),torch.LongTensor(negative_items),torch.DoubleTensor(negative_ratings)
    predr = []
    neg_predr = []
    for i in range(len(tu)):
      rating_min = embed_item_s[ti[i]]
      rating_user = embed_user_s[tu[i]]
      exp_item = torch.FloatTensor(rating_min)
      exp_user = torch.FloatTensor(rating_user)
      b = exp_user * exp_item
      b = b.sum(-1)
      pred_exposure = torch.poisson(b)

      t = torch.FloatTensor([1.0])  
      predr.append(model.forward(tu[i],ti[i],t,pred_exposure).cpu().detach().numpy())
    for i in range(len(nu)):
      rating_min = embed_item_s[ni[i]]
      rating_user = embed_user_s[nu[i]]
      exp_item = torch.FloatTensor(rating_min)
      exp_user = torch.FloatTensor(rating_user)
      b = exp_user * exp_item
      b = b.sum(-1)
      pred_exposure = torch.poisson(b)
        
      t = torch.FloatTensor([1.0])
      neg_predr.append(model.forward(nu[i],ni[i],t,pred_exposure).cpu().detach().numpy())
    subjects = [tu,ti,predr,nu,ni,neg_predr]
    test_users, test_items, test_scores = subjects[0], subjects[1], subjects[2] 
    neg_users, neg_items, neg_scores = subjects[3], subjects[4], subjects[5]
    # the golden set
    test = pd.DataFrame({'user': test_users,
                        'test_item': test_items,
                        'test_score': test_scores})
    # the full set
    full = pd.DataFrame({'user':  np.append(neg_users,test_users),
                      'item':  np.append(neg_items,test_items),
                      'score':  np.append(neg_scores,test_scores)})
    full = pd.merge(full, test, on=['user'], how='left')
    # rank the items according to the scores for each user
    full['rank'] = full.groupby('user')['score'].rank(method='first', ascending=False)
    full.sort_values(['user', 'rank'], inplace=True)
    subjects = full
    print("Hit Ratio is: ",cal_hit_ratio(subjects,10))
    print("NDCG Value is: ",cal_ndcg(subjects,10))
    del full

