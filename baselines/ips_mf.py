# -*- coding: utf-8 -*-
"""IPS_MF.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1muSPDqTJ-1xPunGkUmFnYpSLAvEcqaAH
"""

!pip install hpfrec

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from random import randint
from collections import defaultdict
from sklearn.preprocessing import StandardScaler
import datetime
import csv
import numpy as np
import random
import torch
import torch.nn as nn
import warnings
import pandas as pd
import torch.nn.functional as F
from copy import deepcopy
import pandas as pd
import numpy as np
from hpfrec import HPF

torch.manual_seed(2021)

ratings = pd.read_csv("/content/drive/MyDrive/Copy of Epinions/Data-final/ratings_data.csv")
#ratings = pd.read_csv("/content/drive/MyDrive/Copy of Epinions/Ciao/ratings_data.csv")
ratings = ratings[['userId', 'productId', 'rating']]
ratingsv = ratings[['userId', 'productId', 'rating']].values

print(len(np.unique(ratingsv[:, 0])), len(np.unique(ratingsv[:, 1])))

train = pd.read_csv("/content/drive/MyDrive/Copy of Epinions/testData_Final/ratings_data_train.csv")
#train = pd.read_csv("/content/drive/MyDrive/Copy of Epinions/Ciao/ratings_data_train.csv")
train = train[["userId","productId","rating"]]

print(train.shape)

from scipy.sparse import csc_matrix
mat_data_rate = csc_matrix((len(np.unique(ratingsv[:, 0])), len(np.unique(ratingsv[:, 1]))), dtype=np.int8)
mat_data_rate = mat_data_rate.tolil()
print(mat_data_rate.shape)
for uid,pid,_ in ratingsv:
  mat_data_rate[uid,pid] = _
mat_data_rate = mat_data_rate.tocoo()

class Popularity():

    def __init__(self ,config):
        self.num_users = config['num_users']
        self.num_items = config['num_items']

    def fit(self,train_data):
        column_names = train_data.columns
        userid = column_names[0]
        itemid = column_names[1]
        ratings = column_names[2]
        self.counts_df = train_data.rename(columns = {userid:"UserId",itemid:"productId",ratings:"Count"})
        self.counts_df["Count"] = 1
        self.counts_df['popularity_score'] = self.counts_df.groupby(["productId"])['Count'].transform(sum) / self.num_users
        self.counts_df = self.counts_df[['productId','popularity_score']].drop_duplicates().set_index('productId').to_dict()


    def predict(self,test_data):
        column_names = test_data.columns
        userid = column_names[0]
        itemid = column_names[1]
        ratings = column_names[2]
        self.test_data = test_data.rename(columns = {userid:"UserId",itemid:"productId",ratings:"Count"})
        prediction = self.test_data['productId'].map(self.counts_df['popularity_score']).fillna(1/(self.num_items+1)).values
        return prediction

class Propensity():

    def __init__(self ,config):
        self.num_users = config['num_users']
        self.num_items = config['num_items']
        self.method = config['propensity_model']
        if self.method == 'poisson':
            self.model = HPF(k=10,check_every=10, ncores=-1, maxiter=150)
        if self.method == 'popularity':
            self.model = Popularity(config)

    def fit(self,train_data):
        column_names = train_data.columns
        userid = column_names[0]
        itemid = column_names[1]
        ratings = column_names[2]
        self.counts_df = train_data.rename(columns = {userid:"UserId",itemid:"ItemId",ratings:"Count"})
        self.counts_df["Count"] = 1
        self.model.fit(self.counts_df)

    def predict(self,test_data):
        column_names = test_data.columns
        userid = column_names[0]
        itemid = column_names[1]
        ratings = column_names[2]
        self.test_data = test_data.rename(columns = {userid:"UserId",itemid:"ItemId",ratings:"Count"})
        if self.method == 'poisson':
            prediction = self.model.predict(self.test_data["UserId"].values,self.test_data["ItemId"].values)
            return 1 - np.exp(-prediction)
        else:
            prediction = self.model.predict(self.test_data)
            return prediction

config = {"num_users":40163,"num_items":139738,"latent_dim":64,"batch_size":1000,"propensity_model":"poisson","test_ratio":0.0,"prop":True}
propensity = Propensity(config)
propensity.fit(ratings)

popularity = Popularity(config)
popularity.fit(ratings)

df2 = popularity.predict(train)
df3 = propensity.predict(train)
print(df2.shape)
print(df3.shape)
print(np.mean(1/(1-df2)))
print(np.mean(1/(1-df3)))

from torch.utils.data import DataLoader,Dataset

class UserItemRatingDataset(Dataset):
    """Wrapper, convert <user, item, rating> Tensor into Pytorch Dataset"""
    def __init__(self, user_tensor, item_tensor, target_tensor,prop_tensor):
        """
        args:
            target_tensor: torch.Tensor, the corresponding rating for <user, item> pair
        """
        self.user_tensor = user_tensor
        self.item_tensor = item_tensor
        self.target_tensor = target_tensor
        self.prop_tensor = prop_tensor

    def __getitem__(self, index):
        if self.prop_tensor == None:
            return self.user_tensor[index], self.item_tensor[index], self.target_tensor[index]
        else:
            return self.user_tensor[index], self.item_tensor[index], self.target_tensor[index], self.prop_tensor[index]

    def __len__(self):
        return self.user_tensor.size(0)

class SampleGenerator(object):

    def __init__(self, ratings,config):
        """
        Modified  from: https://github.com/yihong-chen/neural-collaborative-filtering
        Added batching for validation

        TODO: Add a random splitting
        """


        self.batch_size = config['batch_size']
        self.ratings = ratings
        self.n_users = config['num_users']
        self.n_items = config['num_items']
        self.split_rate_test = config['test_ratio']
        self.train_ratings = ratings

        #self.train_ratings, self.test_ratings = self._split(self.ratings)
        self.isprop = config['prop']

        '''if self.isprop == True: #### if we are using  IPS_MF as model
            prop = Propensity(config)
            prop.fit(self.train_ratings)
            scores = prop.predict(self.ratings)

            self.ratings['prop_score'] = scores
            self.train_ratings['prop_score'] = scores[self.train_ratings.index]'''

    def instance_a_train_loader(self, scores,batch_size):
        """instance train loader for one training epoch"""
        users, items, ratings = self.train_ratings['userId'].tolist(), self.train_ratings['productId'].tolist(), self.train_ratings['rating'].tolist()
        if self.isprop:
            dataset = UserItemRatingDataset(user_tensor=torch.LongTensor(users),
                                            item_tensor=torch.LongTensor(items),
                                            target_tensor=torch.FloatTensor(ratings),
                                            prop_tensor=torch.FloatTensor(scores))
        else:
            dataset = UserItemRatingDataset(user_tensor=torch.LongTensor(users),
                                            item_tensor=torch.LongTensor(items),
                                            target_tensor=torch.FloatTensor(ratings))

        return DataLoader(dataset, batch_size=batch_size, shuffle=True)

import torch
from torch.autograd import Variable

class MatrixFactorization(torch.nn.Module):
  def __init__(self, config):
        super(MatrixFactorization, self).__init__()
        self.config = config
        self.num_users = config['num_users']
        self.num_items = config['num_items']
        self.latent_dim = config['latent_dim']
        self.relu = torch.nn.ReLU()

        self.embedding_user = torch.nn.Embedding(num_embeddings=self.num_users, embedding_dim=self.latent_dim)
        self.embedding_item = torch.nn.Embedding(num_embeddings=self.num_items, embedding_dim=self.latent_dim)
        self.fc1 = torch.nn.Linear(128,64)
        self.fc2 = torch.nn.Linear(64,32)
        self.fc3 = torch.nn.Linear(32,1)
  
  def forward(self, user_indices, item_indices):
        user_indices = user_indices.cuda()
        item_indices = item_indices.cuda()
        user_embedding = self.embedding_user(user_indices)
        item_embedding = self.embedding_item(item_indices)
        #element_product = torch.bmm(user_embedding.unsqueeze(1), item_embedding.unsqueeze(1).permute(0, 2, 1)).squeeze()
        element_product = torch.cat([user_embedding,item_embedding],dim=1)
        element_product = self.relu(self.fc1(element_product))
        element_product = self.relu(self.fc2(element_product))
        element_product = self.fc3(element_product)
        #element_product = torch.mm(user_embedding,item_embedding.T)
        #return self.relu(element_product)
        return self.relu(element_product).squeeze()

#np.savetxt("/content/drive/MyDrive/Copy of Epinions/Data-final/popularity_score_train.txt",df2)
np.savetxt("/content/drive/MyDrive/Copy of Epinions/Ciao/propensity_score_train.txt",df3)

import numpy as np
df3 = np.loadtxt("/content/drive/MyDrive/Copy of Epinions/Data-final/propensity_score_train.txt")
#df3 = np.loadtxt("/content/drive/MyDrive/Copy of Epinions/Ciao/propensity_score_train.txt")

df3.shape

print(min(df3))
print(max(df3))
print(np.mean(df3))

print(min(1/(1-df3)))
print(max(1/(1-df3)))
print(np.mean(1/(1-df3)))

config = {"num_users":40163,"num_items":139738,"latent_dim":64,"batch_size":1000,"propensity_model":"poisson","test_ratio":0.0,"prop":True}
#config = {"num_users":7375,"num_items":105114,"latent_dim":64,"batch_size":1000,"propensity_model":"poisson","test_ratio":0.0,"prop":True}
sample_generator = SampleGenerator(train,config)
train_loader = sample_generator.instance_a_train_loader(df3,1000)

print(min(1/df3))
print(max(1/df3))
a = np.sum(df3)

import dill
model = MatrixFactorization(config)
model = model.cuda()

min_loss = 1e+5
breakout = 1
optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)
loss_fn = torch.nn.MSELoss()
for epoch in range(200):
  total_pred_loss = 0
  for batch_num,batch in enumerate(train_loader):
    user,items,ratings,prop_score = batch[0],batch[1], batch[2],batch[3]
    ratings_pred = model.forward(user,items)
    prop_score = prop_score.cuda()
    ratings = ratings.cuda()

    loss = loss_fn(ratings_pred, ratings)
    loss = loss/torch.sum(prop_score)
    a = 1/torch.sum(prop_score)
    loss = loss/a
    loss.backward()
    optimizer.step()
    optimizer.zero_grad()
    total_pred_loss += loss.item()
  if(round(total_pred_loss/(batch_num+1),2)<round(min_loss/(batch_num+1),2)):
    min_loss = total_pred_loss
    breakout = 0 
  else:
    breakout+=1
  if(breakout==10):
    break
  if(epoch%10==0):
      file = "/content/drive/MyDrive/Copy of Epinions/Ciao/IPS_MF_prop_"+str(epoch)+"_"+str(1e-3)
      torch.save(model.state_dict(),f=file)
      print("Model Saved")
  print("Train prediction loss is ", round((total_pred_loss/(batch_num+1)),2)," for epoch ",epoch)

import os
import dill
model = MatrixFactorization(config)
model = model.cuda()
model.load_state_dict(torch.load("/content/drive/MyDrive/Copy of Epinions/Ciao/IPS_MF_prop_"+str(60)+"_"+str(1e-3),pickle_module=dill))
#model.load_state_dict(torch.load("/content/drive/MyDrive/Copy of Epinions/Models_Final/IPS_MF_prop_90_0.001",pickle_module=dill))
loss_fn = torch.nn.MSELoss()
loss_fn2 = torch.nn.L1Loss()
model.eval()
path = "/content/drive/MyDrive/Copy of Epinions/testData_Final/"
#path = "/content/drive/MyDrive/Copy of Epinions/Ciao/"
files = os.listdir(path)
for f in files:
  if("test" in f):
    print(f)  
    test = pd.read_csv(path+f)
    test = test[["userId","productId","rating"]]
    print(test.shape)
    sample_generator = SampleGenerator(test,config)
    df = propensity.predict(test)
    test_loader = sample_generator.instance_a_train_loader(df,100)
    total_pred_loss = 0
    total_avg_loss = 0
    c=0
    for batch_num,batch in enumerate(test_loader):
        user,items,ratings,prop_score = batch[0],batch[1], batch[2],batch[3]
        ratings_pred = model.forward(user,items)
        prop_score = prop_score.cuda()
        ratings = ratings.cuda()

        loss = loss_fn(ratings_pred, ratings)
        loss2 = loss_fn2(ratings_pred, ratings)
        loss = loss/torch.sum(prop_score)
        a = 1/torch.sum(prop_score)
        loss = loss/a
        total_pred_loss += loss.item()
        total_avg_loss += loss2.item()
    print(round(total_pred_loss/(batch_num+1),2),round(total_avg_loss/(batch_num+1),2))

import os
import math
import warnings
warnings.filterwarnings("ignore")
def _sample_negative(item_map,ratings):
        random.seed(10)
        """return all negative items & 100 sampled negative items"""
        interact_status = ratings.groupby('userId')['productId'].apply(set).reset_index().rename(
            columns={'productId': 'interacted_items'})
        interact_status['negative_samples'] = interact_status['interacted_items'].apply(lambda x: random.sample(set(list(range(len(item_map)))) - x, 10))
        return interact_status[['userId', 'negative_samples']]
        #interact_status['negative_items'] = interact_status['interacted_items'].apply(lambda x: set(list(range(len(item_map)))) - x)
        #interact_status['negative_samples'] = interact_status['negative_items'].apply(lambda x: random.sample(x, 10))
        #return interact_status[['userId', 'negative_items', 'negative_samples']]

def cal_hit_ratio(subjects,topk=10):
        """Hit Ratio @ top_K"""
        full, top_k = subjects,topk
        top_k = full[full['rank']<=top_k]
        test_in_top_k =top_k[top_k['test_item'] == top_k['item']]  # golden items hit in the top_K items
        return len(test_in_top_k) * 1.0 / full['user'].nunique()

def cal_ndcg(subjects,topk=10):
        full, top_k = subjects,topk
        top_k = full[full['rank']<=top_k]
        test_in_top_k =top_k[top_k['test_item'] == top_k['item']]
        test_in_top_k['ndcg'] = test_in_top_k['rank'].apply(lambda x: math.log(2) / math.log(1 + x)) # the rank starts from 1
        return test_in_top_k['ndcg'].sum() * 1.0 / full['user'].nunique()


model.load_state_dict(torch.load("/content/drive/MyDrive/Copy of Epinions/Models_Final/IPS_MF_prop_90_0.001"))
#model.load_state_dict(torch.load("/content/drive/MyDrive/Copy of Epinions/Ciao/IPS_MF_prop_"+str(60)+"_"+str(1e-3)))
path = "/content/drive/MyDrive/Copy of Epinions/testData_Final/"
#path = "/content/drive/MyDrive/Copy of Epinions/Ciao/"
files = os.listdir(path)
loss_fn = torch.nn.MSELoss()
loss2_fn = torch.nn.L1Loss()
model.eval()
for f in files:
  if("test" in f):
    print(f)  
    test = pd.read_csv(path+f)
    test = test[["userId","productId","rating"]]
    print(test.shape)
    test_negatives = _sample_negative(list(test["productId"].unique()),test)
    a = pd.merge(test, test_negatives[['userId', 'negative_samples']], on='userId')
    del test_negatives
    test_users,test_items,ratings,negative_users, negative_items,negative_ratings =[],[],[],[],[],[]
    for row in a.itertuples():
      test_users.append(int(row.userId))
      test_items.append(int(row.productId))
      ratings.append(float(row.rating))
      for i in range(len(row.negative_samples)):
          negative_users.append(int(row.userId))
          negative_items.append(int(row.negative_samples[i]))
          negative_ratings.append(float(0))  # negative samples get 0 rating
    tu,ti,tr,nu,ni,nr = torch.LongTensor(test_users), torch.LongTensor(test_items), torch.DoubleTensor(ratings), torch.LongTensor(negative_users),torch.LongTensor(negative_items),torch.DoubleTensor(negative_ratings)
    predr = []
    neg_predr = []
    for i in range(len(tu)):
        user = torch.LongTensor([tu[i]]).cuda()
        item = torch.LongTensor([ti[i]]).cuda()
        predr.append(model.forward(user,item).cpu().detach().numpy())
    for i in range(len(nu)):
        user = torch.LongTensor([nu[i]]).cuda()
        item = torch.LongTensor([ni[i]]).cuda()
        neg_predr.append(model.forward(user,item).cpu().detach().numpy())
    subjects = [tu,ti,predr,nu,ni,neg_predr]
    test_users, test_items, test_scores = subjects[0], subjects[1], subjects[2] 
    neg_users, neg_items, neg_scores = subjects[3], subjects[4], subjects[5]
    # the golden set
    test = pd.DataFrame({'user': test_users,
                        'test_item': test_items,
                        'test_score': test_scores})
    # the full set
    full = pd.DataFrame({'user':  np.append(neg_users,test_users),
                      'item':  np.append(neg_items,test_items),
                      'score':  np.append(neg_scores,test_scores)})
    full = pd.merge(full, test, on=['user'], how='left')
    # rank the items according to the scores for each user
    full['rank'] = full.groupby('user')['score'].rank(method='first', ascending=False)
    full.sort_values(['user', 'rank'], inplace=True)
    subjects = full
    print("Hit Ratio is: ",cal_hit_ratio(subjects,10))
    print("NDCG Value is: ",cal_ndcg(subjects,10))
    del full

