# -*- coding: utf-8 -*-
"""PMF_Baseline.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Em_7ZOr59qP0PFukLQPddxo8rnMgkf7X
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from random import randint
from collections import defaultdict
from sklearn.preprocessing import StandardScaler
import datetime
import csv
import numpy as np
import random
import torch
import torch.nn as nn
import warnings
import pandas as pd
import torch.nn.functional as F
from copy import deepcopy

torch.manual_seed(2021)

ratings = pd.read_csv("/content/drive/MyDrive/Copy of Epinions/Data-final/ratings_data.csv")
#ratings = pd.read_csv("/content/drive/MyDrive/Copy of Epinions/Ciao/ratings_data.csv")
ratings = ratings[['userId', 'productId', 'rating']].values

print(len(np.unique(ratings[:, 0])), len(np.unique(ratings[:, 1])))

train = pd.read_csv("/content/drive/MyDrive/Copy of Epinions/testData_Final/ratings_data_train.csv")
#train = pd.read_csv("/content/drive/MyDrive/Copy of Epinions/Ciao/ratings_data_train.csv")
train = train[["userId","productId","rating"]].values

print(train.shape)

from torch.utils.data import DataLoader,Dataset

class UserItemRatingDataset(Dataset):
    """Wrapper, convert <user, item, rating> Tensor into Pytorch Dataset"""
    def __init__(self, user_tensor, item_tensor, target_tensor):
        """
        args:
            target_tensor: torch.Tensor, the corresponding rating for <user, item> pair
        """
        self.user_tensor = user_tensor
        self.item_tensor = item_tensor
        self.target_tensor = target_tensor

    def __getitem__(self, index):
      get_output = [self.user_tensor[index], self.item_tensor[index], self.target_tensor[index]]

      return get_output

    def __len__(self):
        return self.user_tensor.size(0)

def instance_a_train_loader(data, batch_size):
        users = []
        items = []
        ratings = []

        for row in data:
            users.append(int(row[0]))
            items.append(int(row[1]))
            ratings.append(float(row[2]))
        dataset = UserItemRatingDataset(user_tensor=torch.LongTensor(users),
                                        item_tensor=torch.LongTensor(items),
                                        target_tensor=torch.FloatTensor(ratings),
                                        )
        
        return DataLoader(dataset, batch_size=batch_size, shuffle=True)

train_loader = instance_a_train_loader(train,1000)

from __future__ import print_function
import torch
import torch.utils.data
import torch.nn as nn
import torch.optim as optim
from torch.autograd import Variable
import os
import torch.nn.init as initilize
import pickle
from numpy.random import RandomState
import numpy as np

class PMF(nn.Module):
	def __init__(self, n_users, n_items, n_factors=20, is_sparse=False, no_cuda=None):
		super(PMF, self).__init__()
		self.n_users = n_users
		self.n_items = n_items
		self.n_factors = n_factors
		self.no_cuda = no_cuda
		self.random_state = RandomState(1)

		self.user_embeddings = nn.Embedding(n_users, n_factors, sparse=is_sparse)
		self.user_embeddings.weight.data = torch.from_numpy(0.1 * self.random_state.rand(n_users, n_factors)).float()

		self.item_embeddings = nn.Embedding(n_items, n_factors, sparse=is_sparse)
		self.item_embeddings.weight.data = torch.from_numpy(0.1 * self.random_state.rand(n_items, n_factors)).float()


		self.relu = nn.ReLU()

	def forward(self, users_index, items_index):
		user_h1 = self.user_embeddings(users_index)
		item_h1 = self.item_embeddings(items_index)
		R_h = (user_h1 * item_h1).sum(-1)

		return R_h



	def __call__(self, *args):
		return self.forward(*args)


	def predict(self, users_index, items_index):
		preds = self.forward(users_index, items_index)
		return preds

import dill

model = PMF(40163,139738,64)
#model = PMF(7375,105114,64)
model = model.cuda()

min_loss = 1e+5
breakout = 1
optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)
loss_fn = torch.nn.MSELoss()
loss_fn2 = torch.nn.L1Loss()
for epoch in range(200):
  total_pred_loss = 0
  total_pred_loss2 = 0
  for batch_num,batch in enumerate(train_loader):
    user = batch[0]
    item = batch[1]
    rating = batch[2]
    user = user.cuda()
    item = item.cuda()
    rating = rating.cuda()


    y_pred = model.forward(user,item)
    loss = loss_fn(y_pred,rating)  
    loss.backward()
    optimizer.step()
    optimizer.zero_grad()

    loss2 = loss_fn2(y_pred,rating)
    total_pred_loss += loss.item()
    total_pred_loss2 += loss2.item()
  if(round(total_pred_loss/(batch_num+1),2)<round(min_loss/(batch_num+1),2)):
    min_loss = total_pred_loss
    breakout = 0
  else:
    breakout+=1
  if(breakout==10):
    break
  if(epoch%10==0):
      file = "/content/drive/MyDrive/Copy of Epinions/Models_Final/PMF_"+str(epoch)+"_"+str(1e-3)
      torch.save(model.state_dict(),f=file)
      print("Model Saved")
  print("Train prediction loss is ", round(total_pred_loss/(batch_num+1),2)," and L1 loss is ",round(total_pred_loss2/(batch_num+1),2)," for epoch ",epoch)

import dill
import os
#model = SigNet(len(np.unique(ratings[:, 0])),64,embed_user_s,embed_item_s)
#model = model.cuda()
#model.load_state_dict(torch.load("/content/drive/MyDrive/Copy of Epinions/Ciao/Models/disentangled_rep_"+str(40)+"_"+str(1e-3)))
#path = "/content/drive/MyDrive/Copy of Epinions/Ciao/"
path = "/content/drive/MyDrive/Copy of Epinions/testData_Final/"
files = os.listdir(path)
loss_fn = torch.nn.MSELoss()
loss_fn2 = torch.nn.L1Loss()
all_true_ratings = []
all_pred_ratings = []
model.eval()
c=0
for f in files:
  c+=1
  if(c==51):
    break
  if("test" in f):
    print(f)  
    test = pd.read_csv(path+f)
    test = test[["userId","productId","rating"]].values
    print(test.shape)
    test_loader = instance_a_train_loader(test,100)
    #model = checkpoint_model
    total_pred_loss = 0
    total_pred_loss2 = 0
    for batch_num,batch in enumerate(test_loader):
        user = batch[0]
        u_value = [i.item() for i in user]
        item = batch[1]
        rating = batch[2]
        user = user.cuda()
        item = item.cuda()
        rating = rating.cuda()

        y_pred = model.forward(user,item)
        loss = loss_fn(y_pred,rating)
        loss2 = loss_fn2(y_pred,rating)
        total_pred_loss += loss.item()
        total_pred_loss2 += loss2.item()
        all_true_ratings.extend(rating)
        all_pred_ratings.extend(y_pred)
    print(round(total_pred_loss/(batch_num+1),2),round(total_pred_loss2/(batch_num+1),2))

import os
import math
import warnings
warnings.filterwarnings("ignore")
def _sample_negative(item_map,ratings):
        random.seed(10)
        """return all negative items & 100 sampled negative items"""
        interact_status = ratings.groupby('userId')['productId'].apply(set).reset_index().rename(
            columns={'productId': 'interacted_items'})
        interact_status['negative_samples'] = interact_status['interacted_items'].apply(lambda x: random.sample(set(list(range(len(item_map)))) - x, 10))
        return interact_status[['userId', 'negative_samples']]
        #interact_status['negative_items'] = interact_status['interacted_items'].apply(lambda x: set(list(range(len(item_map)))) - x)
        #interact_status['negative_samples'] = interact_status['negative_items'].apply(lambda x: random.sample(x, 10))
        #return interact_status[['userId', 'negative_items', 'negative_samples']]

def cal_hit_ratio(subjects,topk=10):
        """Hit Ratio @ top_K"""
        full, top_k = subjects,topk
        top_k = full[full['rank']<=top_k]
        test_in_top_k =top_k[top_k['test_item'] == top_k['item']]  # golden items hit in the top_K items
        return len(test_in_top_k) * 1.0 / full['user'].nunique()

def cal_ndcg(subjects,topk=10):
        full, top_k = subjects,topk
        top_k = full[full['rank']<=top_k]
        test_in_top_k =top_k[top_k['test_item'] == top_k['item']]
        test_in_top_k['ndcg'] = test_in_top_k['rank'].apply(lambda x: math.log(2) / math.log(1 + x)) # the rank starts from 1
        return test_in_top_k['ndcg'].sum() * 1.0 / full['user'].nunique()



model.load_state_dict(torch.load("/content/drive/MyDrive/Copy of Epinions/Models_Final/PMF_40_0.001"))
path = "/content/drive/MyDrive/Copy of Epinions/testData_Final/"
#path = "/content/drive/MyDrive/Copy of Epinions/Ciao/"
files = os.listdir(path)
loss_fn = torch.nn.MSELoss()
loss2_fn = torch.nn.L1Loss()
model.eval()
for f in files:
  if("test" in f):
    print(f)  
    test = pd.read_csv(path+f)
    test = test[["userId","productId","rating"]]
    print(test.shape)
    test_negatives = _sample_negative(list(test["productId"].unique()),test)
    a = pd.merge(test, test_negatives[['userId', 'negative_samples']], on='userId')
    del test_negatives
    test_users,test_items,ratings,negative_users, negative_items,negative_ratings =[],[],[],[],[],[]
    for row in a.itertuples():
      test_users.append(int(row.userId))
      test_items.append(int(row.productId))
      ratings.append(float(row.rating))
      for i in range(len(row.negative_samples)):
          negative_users.append(int(row.userId))
          negative_items.append(int(row.negative_samples[i]))
          negative_ratings.append(float(0))  # negative samples get 0 rating
    tu,ti,tr,nu,ni,nr = torch.LongTensor(test_users), torch.LongTensor(test_items), torch.DoubleTensor(ratings), torch.LongTensor(negative_users),torch.LongTensor(negative_items),torch.DoubleTensor(negative_ratings)
    predr = []
    neg_predr = []
    for i in range(len(tu)):
        user = torch.LongTensor([tu[i]]).cuda()
        item = torch.LongTensor([ti[i]]).cuda()
        predr.append(model.forward(user,item).cpu().detach().numpy())
    for i in range(len(nu)):
        user = torch.LongTensor([nu[i]]).cuda()
        item = torch.LongTensor([ni[i]]).cuda()
        neg_predr.append(model.forward(user,item).cpu().detach().numpy())
    subjects = [tu,ti,predr,nu,ni,neg_predr]
    test_users, test_items, test_scores = subjects[0], subjects[1], subjects[2] 
    neg_users, neg_items, neg_scores = subjects[3], subjects[4], subjects[5]
    # the golden set
    test = pd.DataFrame({'user': test_users,
                        'test_item': test_items,
                        'test_score': test_scores})
    # the full set
    full = pd.DataFrame({'user':  np.append(neg_users,test_users),
                      'item':  np.append(neg_items,test_items),
                      'score':  np.append(neg_scores,test_scores)})
    full = pd.merge(full, test, on=['user'], how='left')
    # rank the items according to the scores for each user
    full['rank'] = full.groupby('user')['score'].rank(method='first', ascending=False)
    full.sort_values(['user', 'rank'], inplace=True)
    subjects = full
    print("Hit Ratio is: ",cal_hit_ratio(subjects,10))
    print("NDCG Value is: ",cal_ndcg(subjects,10))
    del full

