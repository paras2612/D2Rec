# -*- coding: utf-8 -*-
"""Disentangled_Rep_Baseline.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dgZp1DQz4Pi3z-8lUdjR0bPIswilSSqi
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from random import randint
from collections import defaultdict
from sklearn.preprocessing import StandardScaler
import datetime
import csv
import numpy as np
import random
import torch
import torch.nn as nn
import warnings
import pandas as pd
import torch.nn.functional as F
from copy import deepcopy

torch.manual_seed(2021)

ratings = pd.read_csv("/content/drive/MyDrive/Copy of Epinions/Data-final/ratings_data.csv")
#ratings = pd.read_csv("/content/drive/MyDrive/Copy of Epinions/Ciao/ratings_data.csv")
ratings = ratings[['userId', 'productId', 'rating']].values

print(len(np.unique(ratings[:, 0])), len(np.unique(ratings[:, 1])))

train = pd.read_csv("/content/drive/MyDrive/Copy of Epinions/testData_Final/ratings_data_train.csv")
#train = pd.read_csv("/content/drive/MyDrive/Copy of Epinions/Ciao/ratings_data_train.csv")
train = train[["userId","productId","rating"]].values

print(train.shape)

from scipy.sparse import csc_matrix
mat_data = csc_matrix((len(np.unique(ratings[:, 0])), len(np.unique(ratings[:, 1]))), dtype=np.int8)
mat_data = mat_data.tolil()
print(mat_data.shape)
for uid,pid,_ in ratings:
  mat_data[uid,pid] = 1
mat_data = mat_data.tocoo()

from scipy.sparse import csc_matrix
mat_data_rate = csc_matrix((len(np.unique(ratings[:, 0])), len(np.unique(ratings[:, 1]))), dtype=np.int8)
mat_data_rate = mat_data_rate.tolil()
print(mat_data_rate.shape)
for uid,pid,_ in ratings:
  mat_data_rate[uid,pid] = _
mat_data_rate = mat_data_rate.tocoo()

from torch.utils.data import DataLoader,Dataset

class UserItemRatingDataset(Dataset):
    """Wrapper, convert <user, item, rating> Tensor into Pytorch Dataset"""
    def __init__(self, user_tensor, item_tensor, target_tensor):
        """
        args:
            target_tensor: torch.Tensor, the corresponding rating for <user, item> pair
        """
        self.user_tensor = user_tensor
        self.item_tensor = item_tensor
        self.target_tensor = target_tensor

    def __getitem__(self, index):
      get_output = [self.user_tensor[index], self.item_tensor[index], self.target_tensor[index]]

      return get_output

    def __len__(self):
        return self.user_tensor.size(0)

def instance_a_train_loader(data, batch_size):
        users = []
        items = []
        ratings = []

        for row in data:
            users.append(int(row[0]))
            items.append(int(row[1]))
            ratings.append(float(row[2]))
        dataset = UserItemRatingDataset(user_tensor=torch.LongTensor(users),
                                        item_tensor=torch.LongTensor(items),
                                        target_tensor=torch.FloatTensor(ratings),
                                        )
        
        return DataLoader(dataset, batch_size=batch_size, shuffle=True)

train_loader = instance_a_train_loader(train,1000)

#embed_user_s = np.loadtxt("/content/drive/MyDrive/Copy of Epinions/Data-final/signedUserEmbedding_new.txt")
#embed_user_s = np.loadtxt("/content/drive/MyDrive/Copy of Epinions/Data-final/signedUserEmbedding_snea.txt")
#embed_user_s = np.loadtxt("/content/drive/MyDrive/Copy of Epinions/Data-final/unsigned_user_embeddings_final11.txt")
#embed_user_s = np.loadtxt("/content/drive/MyDrive/Copy of Epinions/Data-final/unsigned_user_embeddings_final11.txt")

#embed_user_s = np.loadtxt("/content/drive/MyDrive/Copy of Epinions/Data-final/signedUserEmbedding_new.txt")


#embed_user_s = np.loadtxt("/content/drive/MyDrive/Copy of Epinions/Ciao/unsigned_user_embeddings.txt")
embed_user_s = np.loadtxt("/content/drive/MyDrive/Copy of Epinions/Data-final/unsigned_user_embeddings_final100.txt")
embed_user_s = torch.FloatTensor(embed_user_s)
embed_user_s.shape

#embed_item_s = np.loadtxt("/content/drive/MyDrive/Copy of Epinions/Data-final/unsigned_item_embeddings_50.txt")

#embed_item_s = np.loadtxt("/content/drive/MyDrive/Copy of Epinions/Data-final/unsigned_item_embeddings_graphrec.txt")
#embed_item_s = np.loadtxt("/content/drive/MyDrive/Copy of Epinions/Data-final/unsigned_item_embeddings_100.txt")


#embed_item_s = np.loadtxt('/content/drive/MyDrive/Copy of Epinions/Ciao/unsigned_item_embeddings_new.txt')
embed_item_s = np.loadtxt("/content/drive/MyDrive/Copy of Epinions/Data-final/unsigned_item_embeddings.txt")
embed_item_s = torch.FloatTensor(embed_item_s)
embed_item_s.shape

embed_up = torch.rand(len(np.unique(ratings[:, 0])),64).uniform_(torch.min(embed_user_s),torch.max(embed_user_s,unbiased=False))
embed_ip = torch.rand(len(np.unique(ratings[:, 1])),64).uniform_(torch.min(embed_item_s),torch.max(embed_item_s,unbiased=False))

print(embed_up.shape)
print(embed_ip.shape)

embed_up = torch.rand(len(np.unique(ratings[:, 0])),64)
#.uniform_(torch.min(embed_user_s),torch.max(embed_user_s,unbiased=False))
embed_ip = torch.rand(len(np.unique(ratings[:, 1])),64)
#.uniform_(torch.min(embed_item_s),torch.max(embed_item_s,unbiased=False))

print(embed_up.shape)
print(embed_ip.shape)

embed_user_s = np.load("/content/drive/MyDrive/Copy of Epinions/Data-final/random_user_embedding.txt.npy")
embed_item_s = np.load("/content/drive/MyDrive/Copy of Epinions/Data-final/random_item_embedding.npy")

print(embed_user_s.shape)
print(embed_item_s.shape)

np.save("/content/drive/MyDrive/Copy of Epinions/Data-final/random_user_embedding.txt",embed_up)
np.save("/content/drive/MyDrive/Copy of Epinions/Data-final/random_item_embedding.txt",embed_ip)

import torch
from torch.autograd import Variable

class SigNet(torch.nn.Module):
  def __init__(self,n_users,n_features,embed_u,embed_i,disc,kernel_nums=5,kernel_mul=2.0):
    super(SigNet, self).__init__()
    self.theta_u = torch.nn.Parameter(embed_u,requires_grad = True).cuda()
    self.beta_i = torch.nn.Parameter(embed_i,requires_grad = True).cuda()
    self.relu = torch.nn.ReLU()
    self.upsilon = torch.nn.Parameter(torch.rand(n_users),requires_grad=True)
    self.sigmoid = torch.nn.Sigmoid()
    self.kernel_num = kernel_nums
    self.kernel_mul = kernel_mul
    self.fix_sigma = None
    #self.kappa = torch.nn.Parameter(disc,requires_grad=True)
    self.kappa = disc
    self.alpha_u_layer = torch.nn.Linear(n_features,n_features).cuda()
    self.alpha_i_layer = torch.nn.Linear(n_features,n_features).cuda()
    self.gamma_u_layer = torch.nn.Linear(n_features,n_features).cuda()
    self.gamma_i_layer = torch.nn.Linear(n_features,n_features).cuda()
    self.delta_u_layer = torch.nn.Linear(n_features,n_features).cuda()
    self.delta_i_layer = torch.nn.Linear(n_features,n_features).cuda()
    self.alpha_u,self.alpha_i,self.gamma_u,self.gamma_i,self.delta_u,self.delta_i = self.get_alpha_gamma_delta()
    #
    print(self.alpha_u-self.delta_u)
    self.alpha_u = torch.nn.Parameter(self.alpha_u,requires_grad=True)
    self.alpha_i = torch.nn.Parameter(self.alpha_i,requires_grad=True)
    self.gamma_u = torch.nn.Parameter(self.gamma_u,requires_grad=True)
    self.gamma_i = torch.nn.Parameter(self.gamma_i,requires_grad=True)
    self.delta_u = torch.nn.Parameter(self.delta_u,requires_grad=True)
    self.delta_i = torch.nn.Parameter(self.delta_i,requires_grad=True)
  
  def calculate_disc_loss(self,x, y, kernel="rbf"):
    """Emprical maximum mean discrepancy. The lower the result
       the more evidence that distributions are the same.

    Args:
        x: first sample, distribution P
        y: second sample, distribution Q
        kernel: kernel type such as "multiscale" or "rbf"
    """
    xx, yy, zz = torch.mm(x, x.t()), torch.mm(y, y.t()), torch.mm(x, y.t())
    rx = (xx.diag().unsqueeze(0).expand_as(xx))
    ry = (yy.diag().unsqueeze(0).expand_as(yy))
    
    dxx = rx.t() + rx - 2. * xx # Used for A in (1)
    dyy = ry.t() + ry - 2. * yy # Used for B in (1)
    dxy = rx.t() + ry - 2. * zz # Used for C in (1)
    
    XX, YY, XY = (torch.zeros(xx.shape).cuda(),
                  torch.zeros(xx.shape).cuda(),
                  torch.zeros(xx.shape).cuda())
    
    if kernel == "multiscale":
      bandwidth_range = [0.2, 0.5, 0.9, 1.3]
      for a in bandwidth_range:
        XX += a**2 * (a**2 + dxx)**-1
        YY += a**2 * (a**2 + dyy)**-1
        XY += a**2 * (a**2 + dxy)**-1    
    elif kernel == "rbf":  
      bandwidth_range = [10, 15, 20, 50]
      for a in bandwidth_range:
        XX += torch.exp(-0.5*dxx/a)
        YY += torch.exp(-0.5*dyy/a)
        XY += torch.exp(-0.5*dxy/a)
    return torch.mean(XX + YY - 2. * XY)

  def get_alpha_gamma_delta(self):
    alpha_u = self.relu(self.alpha_u_layer(self.theta_u))
    alpha_i = self.relu(self.alpha_i_layer(self.beta_i))
    gamma_u = self.relu(self.gamma_u_layer(self.theta_u))
    gamma_i = self.relu(self.gamma_i_layer(self.beta_i))
    delta_u = self.relu(self.delta_u_layer(self.theta_u))
    delta_i = self.relu(self.delta_i_layer(self.beta_i))

    return alpha_u,alpha_i,gamma_u,gamma_i,delta_u,delta_i
    #,

  def forward(self,user,items):
    user = user.cuda()
    items = items.cuda()

    gamma_user = self.gamma_u[user]
    gamma_user = gamma_user.cuda()
    gamma_item = self.gamma_i[items]
    gamma_item = gamma_item.cuda()

    gamma = torch.matmul(gamma_user,gamma_item.T)
    gamma = gamma.cuda()

    delta_user = self.delta_u[user]
    delta_user = delta_user.cuda()
    delta_item = self.delta_i[items]
    delta_item = delta_item.cuda()

    delta = torch.matmul(delta_user,delta_item.T)
    delta = delta.cuda()
    pred_ratings = delta*gamma
    #

    #a = self.kappa[user].cuda()

    alpha_user = self.alpha_u[user]
    alpha_user = alpha_user.cuda()
    alpha_item = self.alpha_i[items]
    alpha_item = alpha_item.cuda()


    alpha = torch.matmul(alpha_user,alpha_item.T)
    alpha = alpha.cuda()

    b = self.upsilon[user].cuda()
    pred_exposure = alpha*gamma
    #
    disc_loss = self.calculate_disc_loss(gamma_user, alpha_user) + self.calculate_disc_loss(gamma_user, delta_user) +  self.calculate_disc_loss(delta_user, alpha_user) + self.calculate_disc_loss(gamma_item, alpha_item) + self.calculate_disc_loss(gamma_item, delta_item) + self.calculate_disc_loss(delta_item, alpha_item)
    
    return (self.relu(pred_ratings),self.sigmoid(b*(pred_exposure)),self.kappa*disc_loss)

import dill

#embed_user_s = torch.FloatTensor(embed_user_s)
#embed_item_s = torch.FloatTensor(embed_item_s)
#model = SigNet(len(np.unique(ratings[:, 0])),64,embed_up,embed_ip)
disc_coeff = torch.FloatTensor([0.5]).cuda()
model = SigNet(len(np.unique(ratings[:, 0])),64,embed_user_s,embed_item_s,disc_coeff)
model = model.cuda()

min_loss = 1e+5
breakout = 1
optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)
loss_fn = torch.nn.MSELoss()
loss2_fn = torch.nn.BCELoss()
for epoch in range(200):
  total_pred_loss = 0
  total_treat_loss = 0
  total_disc_loss = 0
  for batch_num,batch in enumerate(train_loader):
    user = batch[0]
    item = batch[1]
    rating = batch[2]

    exp_batch = mat_data.tocsr()[user, :].tocsc()[:, item]
    exp_batch_tensor = torch.FloatTensor(np.stack(exp_batch.toarray())).cuda()
    true_prob = torch.sum(exp_batch_tensor)/(exp_batch_tensor.shape[0]*exp_batch_tensor.shape[1])

    rate_batch = mat_data_rate.tocsr()[user, :].tocsc()[:, item]
    rate_batch_tensor = torch.FloatTensor(np.stack(rate_batch.toarray())).cuda()
    idx = torch.where(rate_batch_tensor>0)
    rating_new = rate_batch_tensor[idx[0],idx[1]]

    y_pred,treat_pred,disc_loss = model.forward(user,item)
    batch_loss2 = loss2_fn(treat_pred,exp_batch_tensor)
    total_treat_loss += batch_loss2.item()
    #idx = torch.where(exp_batch_tensor>0)
    pred_prob = torch.sum(treat_pred)/(treat_pred.shape[0]*treat_pred.shape[1])
    y_pred = y_pred[idx[0],idx[1]]
    y_pred = y_pred*(1+(true_prob/(1-true_prob))*(pred_prob/(1-pred_prob)))
    batch_loss = loss_fn(y_pred,rating_new)
    loss = batch_loss + batch_loss2
    loss += disc_loss.item()
    loss.backward(retain_graph=True)
    optimizer.step()
    optimizer.zero_grad()
    total_pred_loss += batch_loss.item()
    total_disc_loss +=disc_loss.item()
  if(round(total_pred_loss/(batch_num+1),2)<round(min_loss/(batch_num+1),2)):
    min_loss = total_pred_loss
    breakout = 0 
  else:
    breakout+=1
  if(breakout==10):
    break
  #if(epoch%10==0):
      #file = "/content/drive/MyDrive/disentangled_rep_test_"+str(epoch)+"_"+str(1e-3)
      #torch.save(model.state_dict(),f=file)
      #print("Model Saved")
  print("Train prediction loss is ", round((total_pred_loss/(batch_num+1)),2), "and Exposure prediction loss",round((total_treat_loss/(batch_num+1)),2),"and Discrepancy loss",round((total_disc_loss/(batch_num+1)),2)," for epoch ",epoch)

import dill
import os

#model = SigNet(len(np.unique(ratings[:, 0])),64,embed_user_s,embed_item_s)
#model = model.cuda()
#model.load_state_dict(torch.load("/content/drive/MyDrive/disentangled_rep_test_"+str(20)+"_"+str(1e-3)))

path = "/content/drive/MyDrive/Copy of Epinions/testData_Final/"
#path = "/content/drive/MyDrive/Copy of Epinions/Ciao/"
files = os.listdir(path)
loss_fn = torch.nn.MSELoss()
loss2_fn = torch.nn.L1Loss()
model.eval()
c=0
for f in files:
  c+=1
  if(c==50):
    break
  if("test" in f):  
    test = pd.read_csv(path+f)
    test = test[["userId","productId","rating"]].values
    print(test.shape)
    test_loader = instance_a_train_loader(test,100)
    #model = checkpoint_model
    total_pred_loss = 0
    total_treat_loss = 0
    for batch_num,batch in enumerate(test_loader):
        user = batch[0]
        u_value = [i.item() for i in user]
        item = batch[1]
        rating = batch[2]
        
        exp_batch = mat_data.tocsr()[user, :].tocsc()[:, item]
        exp_batch_tensor = torch.FloatTensor(np.stack(exp_batch.toarray())).cuda()
        true_prob = torch.sum(exp_batch_tensor)/(exp_batch_tensor.shape[0]*exp_batch_tensor.shape[1])


        rate_batch = mat_data_rate.tocsr()[user, :].tocsc()[:, item]
        rate_batch_tensor = torch.FloatTensor(np.stack(rate_batch.toarray())).cuda()
        idx = torch.where(rate_batch_tensor>0)
        rating_new = rate_batch_tensor[idx[0],idx[1]]


        y_pred,treat_pred,_ = model.forward(user,item)
        pred_prob = torch.sum(treat_pred)/(treat_pred.shape[0]*treat_pred.shape[1])
        #idx = torch.where(exp_batch_tensor>0)
        y_pred = y_pred[idx[0],idx[1]]
        y_pred = y_pred*(1+(true_prob/(1-true_prob))*(pred_prob/(1-pred_prob)))
        batch_loss = loss_fn(y_pred,rating_new)
        total_pred_loss += batch_loss.item()

        batch_loss2 = loss2_fn(y_pred,rating_new)
        total_treat_loss += batch_loss2.item()
    print(round(total_pred_loss/(batch_num+1),2),round(total_treat_loss/(batch_num+1),2))

import os
import math
import warnings
warnings.filterwarnings("ignore")
def _sample_negative(item_map,ratings):
        random.seed(10)
        """return all negative items & 100 sampled negative items"""
        interact_status = ratings.groupby('userId')['productId'].apply(set).reset_index().rename(
            columns={'productId': 'interacted_items'})
        interact_status['negative_samples'] = interact_status['interacted_items'].apply(lambda x: random.sample(set(list(range(len(item_map)))) - x, 10))
        return interact_status[['userId', 'negative_samples']]
        #interact_status['negative_items'] = interact_status['interacted_items'].apply(lambda x: set(list(range(len(item_map)))) - x)
        #interact_status['negative_samples'] = interact_status['negative_items'].apply(lambda x: random.sample(x, 10))
        #return interact_status[['userId', 'negative_items', 'negative_samples']]

def cal_hit_ratio(subjects,topk=10):
        """Hit Ratio @ top_K"""
        full, top_k = subjects,topk
        top_k = full[full['rank']<=top_k]
        test_in_top_k =top_k[top_k['test_item'] == top_k['item']]  # golden items hit in the top_K items
        return len(test_in_top_k) * 1.0 / full['user'].nunique()

def cal_ndcg(subjects,topk=10):
        full, top_k = subjects,topk
        top_k = full[full['rank']<=top_k]
        test_in_top_k =top_k[top_k['test_item'] == top_k['item']]
        test_in_top_k['ndcg'] = test_in_top_k['rank'].apply(lambda x: math.log(2) / math.log(1 + x)) # the rank starts from 1
        return test_in_top_k['ndcg'].sum() * 1.0 / full['user'].nunique()


#model.load_state_dict(torch.load("/content/drive/MyDrive/Copy of Epinions/Models_Final/disentangled_rep_final_20_0.001"))
#model.load_state_dict(torch.load("/content/drive/MyDrive/Copy of Epinions/Ciao/Models/disentangled_rep_"+str(20)+"_"+str(1e-3)))
path = "/content/drive/MyDrive/Copy of Epinions/testData_Final/"
#path = "/content/drive/MyDrive/Copy of Epinions/Ciao/"
files = os.listdir(path)
loss_fn = torch.nn.MSELoss()
loss2_fn = torch.nn.L1Loss()
model.eval()
for f in files:
  if("test" in f):
    if(f=="user_ratings_test_new.csv"):
      continue
    print(f)  
    test = pd.read_csv(path+f)
    test = test[["userId","productId","rating"]]
    print(test.shape)
    test_negatives = _sample_negative(list(test["productId"].unique()),test)
    a = pd.merge(test, test_negatives[['userId', 'negative_samples']], on='userId')
    del test_negatives
    test_users,test_items,ratings,negative_users, negative_items,negative_ratings =[],[],[],[],[],[]
    for row in a.itertuples():
      test_users.append(int(row.userId))
      test_items.append(int(row.productId))
      ratings.append(float(row.rating))
      for i in range(len(row.negative_samples)):
          negative_users.append(int(row.userId))
          negative_items.append(int(row.negative_samples[i]))
          negative_ratings.append(float(0))  # negative samples get 0 rating
    tu,ti,tr,nu,ni,nr = torch.LongTensor(test_users), torch.LongTensor(test_items), torch.DoubleTensor(ratings), torch.LongTensor(negative_users),torch.LongTensor(negative_items),torch.DoubleTensor(negative_ratings)
    predr = []
    neg_predr = []
    for i in range(len(tu)):
      y_pred,treat_pred = model.forward(tu[i],ti[i])
      y_pred = y_pred.cpu().detach().numpy()
      pred_prob = treat_pred.cpu().detach().numpy()
      y_pred = y_pred*(1+(pred_prob/(1-pred_prob)))
      predr.append(y_pred)
    for i in range(len(nu)):
      y_pred,treat_pred = model.forward(nu[i],ni[i])
      y_pred = y_pred.cpu().detach().numpy()
      pred_prob = treat_pred.cpu().detach().numpy()
      y_pred = y_pred*(1+(pred_prob/(1-pred_prob)))
      neg_predr.append(y_pred)
    subjects = [tu,ti,predr,nu,ni,neg_predr]
    test_users, test_items, test_scores = subjects[0], subjects[1], subjects[2] 
    neg_users, neg_items, neg_scores = subjects[3], subjects[4], subjects[5]
    # the golden set
    test = pd.DataFrame({'user': test_users,
                        'test_item': test_items,
                        'test_score': test_scores})
    # the full set
    full = pd.DataFrame({'user':  np.append(neg_users,test_users),
                      'item':  np.append(neg_items,test_items),
                      'score':  np.append(neg_scores,test_scores)})
    full = pd.merge(full, test, on=['user'], how='left')
    # rank the items according to the scores for each user
    full['rank'] = full.groupby('user')['score'].rank(method='first', ascending=False)
    full.sort_values(['user', 'rank'], inplace=True)
    subjects = full
    print("Hit Ratio is: ",cal_hit_ratio(subjects,10))
    print("NDCG Value is: ",cal_ndcg(subjects,10))
    del full